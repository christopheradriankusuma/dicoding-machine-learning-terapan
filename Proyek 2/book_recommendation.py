# -*- coding: utf-8 -*-
"""Book Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JMZqh-1vh-bA28XvhqwS8aa3SxcFJv8b

# Data Loading

Data merupakan dataset [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset) dari kaggle
"""

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle
!chmod 400 ~/.kaggle/kaggle.json
!kaggle datasets download -d arashnic/book-recommendation-dataset

!unzip book-recommendation-dataset.zip

import random
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

"""# EDA"""

books_df = pd.read_csv('Books.csv')
ratings_df = pd.read_csv('Ratings.csv')
users_df = pd.read_csv('Users.csv')

"""## Dataset inspection

### Books
"""

books_df

books_df.info()

books_df.describe()

"""Kolom `Year-Of-Publication` seharusnya bertipe integer"""

with pd.option_context('display.max_colwidth', 400):
  display(books_df[~pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce').notnull()])

"""Sepertinya terdapat kesalahan format csv pada 3 baris tersebut"""

books_df.loc[books_df['ISBN'] == '078946697X', 'Book-Title'] = 'DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)'
books_df.loc[books_df['ISBN'] == '078946697X', 'Book-Author'] = 'Michael Teitelbaum'
books_df.loc[books_df['ISBN'] == '078946697X', 'Year-Of-Publication'] = 2000
books_df.loc[books_df['ISBN'] == '078946697X', 'Publisher'] = 'DK Publishing Inc'

books_df.loc[books_df['ISBN'] == '2070426769', 'Book-Title'] = "Peuple du ciel, suivi de 'Les Bergers'"
books_df.loc[books_df['ISBN'] == '2070426769', 'Book-Author'] = 'Jean-Marie Gustave Le Clezio'
books_df.loc[books_df['ISBN'] == '2070426769', 'Year-Of-Publication'] = 2003
books_df.loc[books_df['ISBN'] == '2070426769', 'Publisher'] = 'Gallimard'

books_df.loc[books_df['ISBN'] == '0789466953', 'Book-Title'] = 'DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)'
books_df.loc[books_df['ISBN'] == '0789466953', 'Book-Author'] = 'James Buckley'
books_df.loc[books_df['ISBN'] == '0789466953', 'Year-Of-Publication'] = 2000
books_df.loc[books_df['ISBN'] == '0789466953', 'Publisher'] = 'DK Publishing Inc'

books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].astype(int)

books_df.describe()

"""Terdapat buku yang dipublikasi di tahun 0, yang tidak masuk akal. Akan dicek distribusi buku yang dipublikasikan di tahun 1900 ke atas"""

sns.displot(books_df[books_df['Year-Of-Publication'] > 1900], kde=True)

"""Sepertinya kebanyakan buku dipublikasikan dari tahun 1970 hingga 2010"""

books_df = books_df[(books_df['Year-Of-Publication'] >= 1970) & (books_df['Year-Of-Publication'] <= 2010)]

sns.displot(books_df['Year-Of-Publication'], kde=True)

"""Karena data buku sangatlah banyak, hanya akan diambil 20 ribu buku saja untuk percobaan"""

books_df = books_df.sample(20000, random_state=42)
books_df.shape

"""### Ratings"""

ratings_df

ratings_df.info()

ratings_df.describe()

"""Rating memiliki nilai dengan rentang 0 hingga 10"""

ratings_df.isnull().sum()

sns.countplot(x=ratings_df['Book-Rating'])

"""Buku dengan rating 0 sangatlah banyak"""

ratings_df[ratings_df['Book-Rating'] == 0][['ISBN']].merge(books_df, on='ISBN').groupby('Book-Title').count().sort_values('ISBN', ascending=False)

"""Akan dihapus saja data dengan rating 0"""

ratings_df = ratings_df[ratings_df['Book-Rating'] != 0].reset_index(drop=True)
ratings_df.shape

"""### Users"""

users_df

users_df.info()

users_df.describe()

sns.displot(users_df['Age'], kde=True)

"""# Data preparation

## Books

Tidak membutuhkan `image-url`, sehingga di-drop saja
"""

books_df.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], inplace=True)

books_df.isnull().sum()

books_df.reset_index(drop=True, inplace=True)
books_df.shape

"""## Users"""

users_df.isnull().sum()

"""Mengatasi null values pada kolom `Age`"""

users_df.describe()

"""Terdapat user dengan umur 0 hingga 244, akan diambil user dengan umur 10 - 70"""

users_df = users_df[(users_df['Age'] >= 10) & (users_df['Age'] <= 70)]

"""Banyak kolom `Age` yang null, dari 270 ribu dataset jika didrop 110 ribu, masih banyak yang tersisa, sehingga didrop saja"""

users_df.dropna(inplace=True)

users_df.isnull().sum()

users_df.reset_index(drop=True, inplace=True)
users_df.shape

"""## Merging dataset"""

_df1 = pd.merge(left=ratings_df, right=users_df, on='User-ID')

book_ratings_df = pd.merge(left=_df1, right=books_df, on='ISBN')
book_ratings_df

"""# Modelling

## Content-based Filtering

Content-based filtering akan dilakukan dengan melihat author dari buku dan menggunakan cosine untuk menghitung kemiripan buku
"""

book_author_df = books_df[['Book-Title', 'Book-Author']]
book_author_df

vectorizer = TfidfVectorizer()
vectorizer.fit(book_author_df['Book-Author'])

tfidf_matrix = vectorizer.transform(book_author_df['Book-Author'])
tfidf_matrix.shape

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=vectorizer.get_feature_names_out(),
    index=book_author_df['Book-Title']
).sample(22, axis=1).sample(10, axis=0)

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=book_author_df['Book-Title'], columns=book_author_df['Book-Title'])
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def book_recommendations(book_name, similarity_data=cosine_sim_df, items=book_author_df[['Book-Title', 'Book-Author']], k=5):
  index = similarity_data.loc[:, book_name].to_numpy().argpartition(range(-1, -k, -1))
  
  closest = similarity_data.columns[index[-1:-(k+2):-1]]
  closest = closest.drop(book_name, errors='ignore')

  return pd.DataFrame(closest).merge(items).head(k)

book_index = random.choice(book_author_df.index)
title = book_author_df.iloc[book_index]['Book-Title']
author = book_author_df.iloc[book_index]['Book-Author']

print(f'Similar books to {title} ({author}) are:')

book_recommendations(title, k=10)

"""## Collaborative-based Filtering"""

class RecommenderNet(tf.keras.models.Model):
  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = tf.keras.layers.Embedding(
      num_users,
      embedding_size,
      embeddings_initializer='he_normal',
      embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
    )
    self.user_bias = tf.keras.layers.Embedding(num_users, 1)
    self.resto_embedding = tf.keras.layers.Embedding(
      num_books,
      embedding_size,
      embeddings_initializer='he_normal',
      embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
    )
    self.resto_bias = tf.keras.layers.Embedding(num_books, 1)
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:, 0])
    user_bias = self.user_bias(inputs[:, 0])
    resto_vector = self.resto_embedding(inputs[:, 1])
    resto_bias = self.resto_bias(inputs[:, 1])
 
    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2) 
 
    x = dot_user_resto + user_bias + resto_bias
    
    return tf.nn.sigmoid(x)

isbn_to_id = {v:k for k, v in enumerate(book_ratings_df['ISBN'].unique())}
id_to_isbn = {v:k for k, v in isbn_to_id.items()}
book_ratings_df['book_id'] = book_ratings_df['ISBN'].map(isbn_to_id)

user_to_id = {v:k for k, v in enumerate(book_ratings_df['User-ID'].unique())}
id_to_user = {v:k for k, v in user_to_id.items()}
book_ratings_df['user_id'] = book_ratings_df['User-ID'].map(user_to_id)

df = book_ratings_df.sample(frac=1, random_state=42).copy()
df

X = df[['user_id', 'book_id']].values
y = df['Book-Rating'].values / 10.0

train_size = round(0.1 * len(df))
X_train = X[:-train_size]
y_train = y[:-train_size]
X_test = X[-train_size:]
y_test = y[-train_size:]

model = RecommenderNet(len(user_to_id), len(isbn_to_id), 10)

model.compile(
    loss='mse',
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
)

history = model.fit(
  X_train, y_train,
  epochs=20,
  validation_data=(X_test, y_test)
)

def print_recommendation(user_id, top_n=10):
  books_read = book_ratings_df[book_ratings_df['user_id'] == user_id]['ISBN'].values
  books_not_read = books_df[~books_df['ISBN'].isin(books_read)]['ISBN'].values
  books_not_read_id = [isbn_to_id.get(i) for i in books_not_read]
  inp = np.asarray([[user_id, i] for i in books_not_read_id]).astype('float64')

  ratings = model.predict(inp).flatten()
  recommended_books_id = [id_to_isbn.get(i) for i in ratings.argsort()[-top_n:][::-1]]
  user_top_books = book_ratings_df[book_ratings_df['user_id'] == user_id].sort_values(by='Book-Rating', ascending=False).head(5)

  print(f'User {id_to_user.get(user_id)} has read:')
  for line in user_top_books.values:
    if line[2] > 0:
      print(f'{line[5]} ({line[6]}, {line[7]}). Rating: {line[2]}/10')

  ratings.sort()
  print('-' * 100)
  print(f'Recommended books for user {id_to_user.get(user_id)} are:')
  for rating, line in zip(ratings.flatten()[-top_n:][::-1], books_df[books_df['ISBN'].isin(recommended_books_id)].values):
    print(f'{line[1]} ({line[2]}, {line[3]}). Rating prediction: {round(rating * 10)}/10')

print_recommendation(random.choice(book_ratings_df['user_id']))

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model_metrics')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()